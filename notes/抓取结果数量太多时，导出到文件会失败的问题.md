[Exporting Crawl Results not working](https://github.com/xuejianxianzun/PixivBatchDownloader/issues/131)


抓取结果数量太多时，导出到文件会失败的问题

测试：https://www.pixiv.net/users/973675/following

抓取这个用户关注的所有用户的作品，结果总数约为 353,098。

抓取完成后，该页面内存占用 542MB。

导出 CSV 文件：可以成功导出 CSV 文件，导出时该页面内存占用升到 1035MB（+513），导出完成后回落。（CSV 文件体积 89.2MB）

导出抓取结果（为 json 文件）：导出失败，该页面内存占用峰值升到 1768MB（+1256），然后报错，然后内存回落。（系统内存容量充足，并非是内存用尽）

报错信息：

```
Uncaught RangeError: Invalid string length
at JSON.stringify (<anonymous>)
at Function.json2Blob
```

根据搜索，原因是 V8 引擎对于单个字符串长度有限制，在 64 位操作系统上，可能的限制约为 1GB。当抓取结果太多时，使用 `JSON.stringify` 把所有结果转换成一个字符串，可能超过了长度限制。

资料来源：https://chromium-review.googlesource.com/c/v8/v8/+/2030916

----------

----------

导出 CSV 文件时，如果结果数量更多，会因为字符串长度达到限制而发生错误吗？

目前看来，有这个可能，因为在把 CSV 文件的数据转换成 BLOB 时，把所有数据合并成了字符串：

```ts
result.join(this.CRLF)
```

如果抓取结果太多，这也可能会产生错误。还需要验证。