# taskMeta

抓取结果的元数据，并不保存实际数据。因为实际数据太大的话需要分批存储。


```
{
id:时间戳,
url:'xxxxx',
part:number
}
```

- id 不重复，作为主键。
- url   网址，不包含 hash 部分。一个网址（页面）里只会保存最新一次的抓取结果。
- part  记录数据被分成了几个部分

## 处理时机

抓取完毕时，保存完整任务数据到这个表

下载完毕后，删除这个任务的数据

页面加载时，读取该网址下有没有任务，如果有，则加载这个任务的数据，恢复进度。并且将下载面板显示的选项卡设置为“下载”部分。

# taskData

存储部分或全部的任务数据。

```
{
id:时间戳 + part,
data:result[]
}
```

- id 任务的 id，加上索引号使 id 唯一
- data  这个部分里储存的任务的数据

# states

下载过程中，每下载完一个作品，保存下载状态到表 states 里

```
{
  id:时间戳,
  states:(-1,0,1)[]
}
```

- id 时间戳，使用 taskMeta 的时间戳
- states 本次任务中所有文件的下载状态

下载完毕后，删除这个下载任务的所有数据。

# 压力测试

2020-07-13，抓取我所有的关注用户的作品。

```
当前有 2656 个用户
当前有 420438 个作品
已获取 996270 个文件网址
抓取完毕！
```

抓取约花费了 5 个小时，每秒约完成 24 个请求。

统计不同类型的文件个数：

```
插画  714745
漫画  267419
动画  13370
小说  736
```

没能保存到 IndexedDB 里，IndexedDB 抛出了一个错误事件：

```
Event {
  target: IDBRequest {
    error: DOMException {
      code: 0
      message: "The serialized keys and/or value are too large (size=625726452 bytes, max=133169152 bytes)."
      name: "UnknownError"
    }
  }
}
```

提示信息说序列化之后的键值对太大，625726452 约等于 **600 MiB**，而浏览器限制的最大值 133169152 约等于 **127 MiB**。经查，这个限制 6 年前就有了，这么多年也没加大容量，唉。

这个限制是单条数据的限制，而不是总的体积上限。因此或许麻烦一点，可以考虑分批存储。

分析一下数据的体积：

小说的文件以 blob 类型保存在抓取结果里，所有小说的 blob 体积总共有 `99994522` 字节，约为 95 MiB。

上面 IndexedDB 说的“序列化后”的体积，不知道和这些数据在页面内存中的体积是否一致。如果认为两者相同，则每个抓取结果除 blob 之外的数据体积为 0.5 KiB。但如果在页面内存中的体积更大一些的话，可能为 0.7 - 0.8 KiB。
